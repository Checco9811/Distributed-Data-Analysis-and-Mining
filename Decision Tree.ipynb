{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hired-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"AppName\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comfortable-qatar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('groupId', 'string'),\n",
       " ('matchId', 'string'),\n",
       " ('matchDuration', 'int'),\n",
       " ('winPlaceClass', 'int'),\n",
       " ('isFirstPerson', 'boolean'),\n",
       " ('matchtype', 'int'),\n",
       " ('maxPlace', 'int'),\n",
       " ('numGroups', 'int'),\n",
       " ('assist_SUM', 'int'),\n",
       " ('assist_AVG', 'int'),\n",
       " ('heals_SUM', 'int'),\n",
       " ('heals_AVG', 'int'),\n",
       " ('kills_SUM', 'int'),\n",
       " ('kills_AVG', 'int'),\n",
       " ('headshotKills_SUM', 'int'),\n",
       " ('headshotKills_AVG', 'int'),\n",
       " ('killStreaks_MAX', 'int'),\n",
       " ('roadKills_SUM', 'int'),\n",
       " ('roadKills_AVG', 'int'),\n",
       " ('longestKill_MAX', 'double'),\n",
       " ('vehicleDestroys_MAX', 'int'),\n",
       " ('weaponsAcquired_SUM', 'int'),\n",
       " ('weaponsAcquired_AVG', 'int'),\n",
       " ('damageDealt_SUM', 'double'),\n",
       " ('damageDealt_AVG', 'double'),\n",
       " ('distance_SUM', 'double'),\n",
       " ('distance_AVG', 'double'),\n",
       " ('rideDistance_SUM', 'double'),\n",
       " ('rideDistance_AVG', 'double'),\n",
       " ('swimDistance_SUM', 'double'),\n",
       " ('swimDistance_AVG', 'double'),\n",
       " ('walkDistance_SUM', 'double'),\n",
       " ('walkDistance_AVG', 'double'),\n",
       " ('DBNOs_SUM', 'int'),\n",
       " ('DBNOs_AVG', 'int'),\n",
       " ('revives_SUM', 'int'),\n",
       " ('revives_AVG', 'int'),\n",
       " ('teamKills_SUM', 'int'),\n",
       " ('teamKills_AVG', 'int'),\n",
       " ('killPlace_MAX', 'int'),\n",
       " ('rankPoints_MAX', 'int'),\n",
       " ('killPoints_MAX', 'int'),\n",
       " ('winPoints_MAX', 'int')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "data = spark.read.csv(\"GROUP.csv.gz\", header=True, sep=',',inferSchema=\"true\")\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "molecular-mechanism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['groupId',\n",
       " 'matchId',\n",
       " 'matchDuration',\n",
       " 'winPlaceClass',\n",
       " 'isFirstPerson',\n",
       " 'matchtype',\n",
       " 'maxPlace',\n",
       " 'numGroups',\n",
       " 'assist_SUM',\n",
       " 'assist_AVG',\n",
       " 'heals_SUM',\n",
       " 'heals_AVG',\n",
       " 'kills_SUM',\n",
       " 'kills_AVG',\n",
       " 'headshotKills_SUM',\n",
       " 'headshotKills_AVG',\n",
       " 'killStreaks_MAX',\n",
       " 'roadKills_SUM',\n",
       " 'roadKills_AVG',\n",
       " 'longestKill_MAX',\n",
       " 'vehicleDestroys_MAX',\n",
       " 'weaponsAcquired_SUM',\n",
       " 'weaponsAcquired_AVG',\n",
       " 'damageDealt_SUM',\n",
       " 'damageDealt_AVG',\n",
       " 'distance_SUM',\n",
       " 'distance_AVG',\n",
       " 'rideDistance_SUM',\n",
       " 'rideDistance_AVG',\n",
       " 'swimDistance_SUM',\n",
       " 'swimDistance_AVG',\n",
       " 'walkDistance_SUM',\n",
       " 'walkDistance_AVG',\n",
       " 'DBNOs_SUM',\n",
       " 'DBNOs_AVG',\n",
       " 'revives_SUM',\n",
       " 'revives_AVG',\n",
       " 'teamKills_SUM',\n",
       " 'teamKills_AVG',\n",
       " 'killPlace_MAX',\n",
       " 'rankPoints_MAX',\n",
       " 'killPoints_MAX',\n",
       " 'winPoints_MAX']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fewer-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols =  ['matchDuration', \n",
    " 'isFirstPerson',\n",
    " 'matchtype',\n",
    " 'maxPlace',\n",
    " 'numGroups',\n",
    " 'assist_SUM',\n",
    " 'assist_AVG',\n",
    " 'heals_SUM',\n",
    " 'heals_AVG',\n",
    " 'kills_SUM',\n",
    " 'kills_AVG',\n",
    " 'headshotKills_SUM',\n",
    " 'headshotKills_AVG',\n",
    " 'killStreaks_MAX',\n",
    " 'roadKills_SUM',\n",
    " 'roadKills_AVG',\n",
    " 'longestKill_MAX',\n",
    " 'vehicleDestroys_MAX',\n",
    " 'weaponsAcquired_SUM',\n",
    " 'weaponsAcquired_AVG',\n",
    " 'damageDealt_SUM',\n",
    " 'damageDealt_AVG',\n",
    " 'distance_SUM',\n",
    " 'distance_AVG',\n",
    " 'rideDistance_SUM',\n",
    " 'rideDistance_AVG',\n",
    " 'swimDistance_SUM',\n",
    " 'swimDistance_AVG',\n",
    " 'walkDistance_SUM',\n",
    " 'walkDistance_AVG',\n",
    " 'DBNOs_SUM',\n",
    " 'DBNOs_AVG',\n",
    " 'revives_SUM',\n",
    " 'revives_AVG',\n",
    " 'teamKills_SUM',\n",
    " 'teamKills_AVG',\n",
    " 'killPlace_MAX',\n",
    " 'rankPoints_MAX',\n",
    " 'killPoints_MAX',\n",
    " 'winPoints_MAX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "upset-projection",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# winPlaceClass\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=inputCols,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "unsigned-motorcycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 141432\n",
      "testing: 60000\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "withReplacement=False\n",
    "output = output.sample(withReplacement, 0.1, seed).select(\"features\", \"winPlaceClass\")\n",
    "\n",
    "(training,testing) = output.randomSplit([0.7,0.3])\n",
    "print(F'training: {training.count()}')\n",
    "print(F'testing: {testing.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ruled-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(maxDepth=5, labelCol=\"winPlaceClass\",\n",
    "                            featuresCol=\"features\", impurity=\"gini\",\n",
    "                            seed=42)\n",
    "model = dt.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "musical-texas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|            features|winPlaceClass|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,60.0,2672.0,...|[0.0,0.0089512158...|       3.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,3087.0,23511...|[0.0,0.0969961666...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            1|[0.0,3087.0,23511...|[0.0,0.0969961666...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,3087.0,23511...|[0.0,0.0969961666...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            3|[0.0,1.0,35.0,141...|[0.0,1.2680699974...|       4.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,60.0,2672.0,...|[0.0,0.0089512158...|       3.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,3087.0,23511...|[0.0,0.0969961666...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,3087.0,23511...|[0.0,0.0969961666...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            4|[0.0,8.0,895.0,65...|[0.0,8.1433224755...|       3.0|\n",
      "|(40,[0,1,2,3,4,5,...|            3|[0.0,60.0,2672.0,...|[0.0,0.0089512158...|       3.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,3087.0,23511...|[0.0,0.0969961666...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            3|[0.0,8.0,895.0,65...|[0.0,8.1433224755...|       3.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,3087.0,23511...|[0.0,0.0969961666...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,3087.0,23511...|[0.0,0.0969961666...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,60.0,2672.0,...|[0.0,0.0089512158...|       3.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,3087.0,23511...|[0.0,0.0969961666...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,101.0,3290.0...|[0.0,0.0173271573...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,3087.0,23511...|[0.0,0.0969961666...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            2|[0.0,3087.0,23511...|[0.0,0.0969961666...|       2.0|\n",
      "|(40,[0,1,2,3,4,5,...|            3|[0.0,8.0,895.0,65...|[0.0,8.1433224755...|       3.0|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(testing)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-jacksonville",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "later-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictionAndLabels = predictions.rdd.map(lambda x: (x.prediction, float(x.winPlaceClass)))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "informal-carpet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 0.7353666666666667\n",
      "Precision = 0.7200817833760604\n",
      "F1 measure = 0.7250574298966894\n",
      "Accuracy = 0.7353666666666666\n",
      "Class 1.0 precision = 0.0\n",
      "Class 1.0 recall = 0.0\n",
      "Class 1.0 F1 Measure = 0.0\n",
      "Class 2.0 precision = 0.7100576874843241\n",
      "Class 2.0 recall = 0.8111747851002865\n",
      "Class 2.0 F1 Measure = 0.7572555837902903\n",
      "Class 3.0 precision = 0.6663277278060227\n",
      "Class 3.0 recall = 0.6009876543209877\n",
      "Class 3.0 F1 Measure = 0.6319732937685459\n",
      "Class 4.0 precision = 0.7028687302439315\n",
      "Class 4.0 recall = 0.8039978168917997\n",
      "Class 4.0 F1 Measure = 0.7500397772474144\n",
      "Class 5.0 precision = 0.8711461867532084\n",
      "Class 5.0 recall = 0.8195253955037469\n",
      "Class 5.0 F1 Measure = 0.844547729710404\n",
      "Class 6.0 precision = 0.7250530785562632\n",
      "Class 6.0 recall = 0.5003663003663004\n",
      "Class 6.0 F1 Measure = 0.5921109666233203\n",
      "Confusion Matrix\n",
      "[[0.0000e+00 1.3630e+03 5.9000e+01 3.0000e+00 5.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 1.1324e+04 2.4710e+03 1.6300e+02 2.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 3.1430e+03 8.5190e+03 2.4750e+03 3.8000e+01 0.0000e+00]\n",
      " [0.0000e+00 1.1500e+02 1.7250e+03 1.1785e+04 1.0330e+03 0.0000e+00]\n",
      " [0.0000e+00 1.0000e+00 1.1000e+01 2.3300e+03 1.1811e+04 2.5900e+02]\n",
      " [0.0000e+00 2.0000e+00 0.0000e+00 1.1000e+01 6.6900e+02 6.8300e+02]]\n"
     ]
    }
   ],
   "source": [
    "labels = [1,2,3,4,5,6]\n",
    "# Summary stats\n",
    "print(\"Recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"F1 measure = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Accuracy = %s\" % metrics.accuracy)\n",
    "\n",
    "# Individual label stats\n",
    "labels = [1.0, 2.0 ,3.0, 4.0, 5.0, 6.0]\n",
    "for label in labels:\n",
    "    print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label)))\n",
    "    \n",
    "print('Confusion Matrix')\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-cleaner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-might",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-christianity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-manual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "welcome-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.725057 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"winPlaceClass\",\n",
    "                                              predictionCol=\"prediction\")\n",
    "evaluator.setMetricName('f1')\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "\n",
    "#print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\"F1 = %g \" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "olive-orlando",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.735367 \n"
     ]
    }
   ],
   "source": [
    "evaluator.setMetricName('accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"accuracy = %g \" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "proprietary-celebration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1\n",
      "0.7250574298966894\n",
      "#########################################################\n",
      "accuracy\n",
      "0.7353666666666666\n",
      "#########################################################\n",
      "weightedPrecision\n",
      "0.7200817833760604\n",
      "#########################################################\n",
      "weightedRecall\n",
      "0.7353666666666667\n",
      "#########################################################\n",
      "weightedTruePositiveRate\n",
      "0.7353666666666667\n",
      "#########################################################\n",
      "weightedFalsePositiveRate\n",
      "0.08150905942758307\n",
      "#########################################################\n",
      "weightedFMeasure\n",
      "0.7250574298966894\n",
      "#########################################################\n",
      "truePositiveRateByLabel\n",
      "1 = 0.0\n",
      "2 = 0.8111747851002865\n",
      "3 = 0.6009876543209877\n",
      "4 = 0.8039978168917997\n",
      "5 = 0.8195253955037469\n",
      "6 = 0.5003663003663004\n",
      "#########################################################\n",
      "falsePositiveRateByLabel\n",
      "1 = 0.0\n",
      "2 = 0.10043440486533449\n",
      "3 = 0.09309328968903437\n",
      "4 = 0.10987605310749415\n",
      "5 = 0.03832148811090638\n",
      "6 = 0.0044171569881470115\n",
      "#########################################################\n",
      "precisionByLabel\n",
      "1 = 0.0\n",
      "2 = 0.7100576874843241\n",
      "3 = 0.6663277278060227\n",
      "4 = 0.7028687302439315\n",
      "5 = 0.8711461867532084\n",
      "6 = 0.7250530785562632\n",
      "#########################################################\n",
      "recallByLabel\n",
      "1 = 0.0\n",
      "2 = 0.8111747851002865\n",
      "3 = 0.6009876543209877\n",
      "4 = 0.8039978168917997\n",
      "5 = 0.8195253955037469\n",
      "0.6897883948754812\n",
      "#########################################################\n",
      "hammingLoss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-9b67222ef004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mF'{c} = {evaluator.evaluate(predictions)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#########################################################'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \"\"\"\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = ['f1', 'accuracy',  'weightedPrecision',  'weightedRecall',  'weightedTruePositiveRate', \n",
    "          'weightedFalsePositiveRate',  'weightedFMeasure', 'truePositiveRateByLabel',\n",
    "          'falsePositiveRateByLabel', 'precisionByLabel',  'recallByLabel',  'fMeasureByLabel',  'logLoss',  'hammingLoss']\n",
    "\n",
    "byLabel = ['truePositiveRateByLabel','falsePositiveRateByLabel', 'precisionByLabel',  'recallByLabel',  'fMeasureByLabel']\n",
    "labels = [1,2,3,4,5,6]\n",
    "\n",
    "for metric in metrics:\n",
    "    print(metric)\n",
    "    evaluator.setMetricName(metric)\n",
    "    \n",
    "    if metric in byLabel:\n",
    "        for c in  labels:\n",
    "            evaluator.setMetricLabel(c)\n",
    "            print(F'{c} = {evaluator.evaluate(predictions)}')\n",
    "    else:\n",
    "        print(evaluator.evaluate(predictions))\n",
    "    print('#########################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (f1 | accuracy | weightedPrecision | weightedRecall | weightedTruePositiveRate | \n",
    "#  weightedFalsePositiveRate | weightedFMeasure | truePositiveRateByLabel | \n",
    "# falsePositiveRateByLabel | precisionByLabel | recallByLabel | fMeasureByLabel | \n",
    "# logLoss | hammingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-tutorial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-sphere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "casual-louisiana",
   "metadata": {},
   "source": [
    "## Sklearn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "mechanical-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = predictions.select(['winPlaceClass']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "athletic-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "median-context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00      1430\n",
      "           2       0.71      0.81      0.76     13960\n",
      "           3       0.67      0.60      0.63     14175\n",
      "           4       0.70      0.80      0.75     14658\n",
      "           5       0.87      0.82      0.84     14412\n",
      "           6       0.73      0.50      0.59      1365\n",
      "\n",
      "    accuracy                           0.74     60000\n",
      "   macro avg       0.61      0.59      0.60     60000\n",
      "weighted avg       0.72      0.74      0.73     60000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "latin-pride",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0  1363    59     3     5     0]\n",
      " [    0 11324  2471   163     2     0]\n",
      " [    0  3143  8519  2475    38     0]\n",
      " [    0   115  1725 11785  1033     0]\n",
      " [    0     1    11  2330 11811   259]\n",
      " [    0     2     0    11   669   683]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-singer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
